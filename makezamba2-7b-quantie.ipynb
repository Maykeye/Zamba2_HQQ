{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fba0bef6-d1b7-4a0e-b0c9-0a633173849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to HF model\n",
    "\n",
    "from pathlib import Path\n",
    "MODEL_ID = \"Zyphra_Zamba2-7B-instruct\"\n",
    "MODEL_PATH=Path(f\"~/models/{MODEL_ID}\").expanduser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e0f3d35-7e58-4f40-8127-56f7d2e4f518",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fella/src/sd/sd/lib/python3.12/site-packages/torchao/ops.py:12: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  return torch.library.impl_abstract(f\"{name}\")(func)\n",
      "/home/fella/src/sd/sd/lib/python3.12/site-packages/mamba_ssm/ops/selective_scan_interface.py:163: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/home/fella/src/sd/sd/lib/python3.12/site-packages/mamba_ssm/ops/selective_scan_interface.py:239: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/home/fella/src/sd/sd/lib/python3.12/site-packages/mamba_ssm/ops/triton/layer_norm.py:985: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/home/fella/src/sd/sd/lib/python3.12/site-packages/mamba_ssm/ops/triton/layer_norm.py:1044: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/home/fella/src/sd/sd/lib/python3.12/site-packages/mamba_ssm/distributed/tensor_parallel.py:25: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/home/fella/src/sd/sd/lib/python3.12/site-packages/mamba_ssm/distributed/tensor_parallel.py:61: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/home/fella/src/sd/sd/lib/python3.12/site-packages/mamba_ssm/ops/triton/ssd_combined.py:757: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/home/fella/src/sd/sd/lib/python3.12/site-packages/mamba_ssm/ops/triton/ssd_combined.py:835: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n"
     ]
    }
   ],
   "source": [
    "# Mamba layer was modified to get dtype based on conv1d rather than inputs. \n",
    "import modeling_zamba2\n",
    "import configuration_zamba2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c616bb35-b274-4718-ac1c-b3e35b16465b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hqq.core.quantize import BaseQuantizeConfig, HQQLinear, HQQBackend\n",
    "from hqq.core.optimize import optimize_weights_proximal\n",
    "from hqq.core.quantize import Quantizer\n",
    "# This supposed to produce better results\n",
    "Quantizer.optimize_weights = optimize_weights_proximal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72f15221-1e67-4e46-84bd-57f445e342c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AO4: True\n"
     ]
    }
   ],
   "source": [
    "USE_AOINT4 = True\n",
    "\n",
    "# we will define three level of quants to use later: \n",
    "BASIC_QUANT={'nbits': 4, 'group_size': 64}\n",
    "#BETTER_QUANT={'nbits': 8, 'group_size': 64} -- no aoint4\n",
    "BETTER_QUANT={'nbits': 4, 'group_size': 32}\n",
    "#BETTER_QUANT=BASIC_QUANT\n",
    "NO_QUANT = None\n",
    "AXIS = (1) if USE_AOINT4 else (0)\n",
    "\n",
    "print(f\"AO4: {USE_AOINT4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4cb6d47-a83e-403c-9a84-f5827b0d36e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import time\n",
    "import torch.nn as nn\n",
    "\n",
    "from copy import deepcopy\n",
    "from typing import Optional, Union, Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83f95533-806b-4e70-a0f1-aff185ce90f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zamba2ForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae14843289b94d9e9ee711bb2e7e7675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Zamba2ForCausalLM(\n",
       "  (model): Zamba2Model(\n",
       "    (embed_tokens): Embedding(32000, 3584, padding_idx=0)\n",
       "    (blocks): ModuleList(\n",
       "      (0-1): 2 x Zamba2AttentionDecoderLayer(\n",
       "        (self_attn): Zamba2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=7168, out_features=7168, bias=False)\n",
       "          (k_proj): Linear(in_features=7168, out_features=7168, bias=False)\n",
       "          (v_proj): Linear(in_features=7168, out_features=7168, bias=False)\n",
       "          (o_proj): Linear(in_features=7168, out_features=3584, bias=False)\n",
       "          (rotary_emb): Zamba2RotaryEmbedding()\n",
       "        )\n",
       "        (feed_forward): Zamba2MLP(\n",
       "          (linear_fc1): Linear(in_features=3584, out_features=28672, bias=False)\n",
       "          (linear_fc2): Linear(in_features=14336, out_features=3584, bias=False)\n",
       "          (linear_fc1_lora_A_list): ParameterList(\n",
       "              (0): Object of type: Linear\n",
       "              (1): Object of type: Linear\n",
       "              (2): Object of type: Linear\n",
       "              (3): Object of type: Linear\n",
       "              (4): Object of type: Linear\n",
       "              (5): Object of type: Linear\n",
       "              (6): Object of type: Linear\n",
       "              (7): Object of type: Linear\n",
       "              (8): Object of type: Linear\n",
       "              (9): Object of type: Linear\n",
       "              (10): Object of type: Linear\n",
       "              (11): Object of type: Linear\n",
       "              (12): Object of type: Linear\n",
       "            (0): Linear(in_features=3584, out_features=128, bias=False)\n",
       "            (1): Linear(in_features=3584, out_features=128, bias=False)\n",
       "            (2): Linear(in_features=3584, out_features=128, bias=False)\n",
       "            (3): Linear(in_features=3584, out_features=128, bias=False)\n",
       "            (4): Linear(in_features=3584, out_features=128, bias=False)\n",
       "            (5): Linear(in_features=3584, out_features=128, bias=False)\n",
       "            (6): Linear(in_features=3584, out_features=128, bias=False)\n",
       "            (7): Linear(in_features=3584, out_features=128, bias=False)\n",
       "            (8): Linear(in_features=3584, out_features=128, bias=False)\n",
       "            (9): Linear(in_features=3584, out_features=128, bias=False)\n",
       "            (10): Linear(in_features=3584, out_features=128, bias=False)\n",
       "            (11): Linear(in_features=3584, out_features=128, bias=False)\n",
       "            (12): Linear(in_features=3584, out_features=128, bias=False)\n",
       "          )\n",
       "          (linear_fc1_lora_B_list): ParameterList(\n",
       "              (0): Object of type: Linear\n",
       "              (1): Object of type: Linear\n",
       "              (2): Object of type: Linear\n",
       "              (3): Object of type: Linear\n",
       "              (4): Object of type: Linear\n",
       "              (5): Object of type: Linear\n",
       "              (6): Object of type: Linear\n",
       "              (7): Object of type: Linear\n",
       "              (8): Object of type: Linear\n",
       "              (9): Object of type: Linear\n",
       "              (10): Object of type: Linear\n",
       "              (11): Object of type: Linear\n",
       "              (12): Object of type: Linear\n",
       "            (0): Linear(in_features=128, out_features=28672, bias=False)\n",
       "            (1): Linear(in_features=128, out_features=28672, bias=False)\n",
       "            (2): Linear(in_features=128, out_features=28672, bias=False)\n",
       "            (3): Linear(in_features=128, out_features=28672, bias=False)\n",
       "            (4): Linear(in_features=128, out_features=28672, bias=False)\n",
       "            (5): Linear(in_features=128, out_features=28672, bias=False)\n",
       "            (6): Linear(in_features=128, out_features=28672, bias=False)\n",
       "            (7): Linear(in_features=128, out_features=28672, bias=False)\n",
       "            (8): Linear(in_features=128, out_features=28672, bias=False)\n",
       "            (9): Linear(in_features=128, out_features=28672, bias=False)\n",
       "            (10): Linear(in_features=128, out_features=28672, bias=False)\n",
       "            (11): Linear(in_features=128, out_features=28672, bias=False)\n",
       "            (12): Linear(in_features=128, out_features=28672, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): Zamba2RMSNorm()\n",
       "        (pre_ff_layernorm): Zamba2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (mamba_layers): ModuleList(\n",
       "      (0-80): 81 x Zamba2MambaDecoderLayer(\n",
       "        (mamba): Mamba2Layer(\n",
       "          (in_proj): ModuleList(\n",
       "            (0): Linear(in_features=3584, out_features=14704, bias=False)\n",
       "          )\n",
       "          (conv1d): Conv1d(7424, 7424, kernel_size=(4,), stride=(1,), padding=(3,), groups=7424)\n",
       "          (act): SiLU()\n",
       "          (norm): RMSNorm()\n",
       "          (out_proj): Linear(in_features=7168, out_features=3584, bias=False)\n",
       "        )\n",
       "        (input_layernorm): Zamba2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (linear_layers): ModuleList(\n",
       "      (0-12): 13 x Linear(in_features=3584, out_features=3584, bias=False)\n",
       "    )\n",
       "    (final_layernorm): Zamba2RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = modeling_zamba2.Zamba2ForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    device_map=\"cuda\", \n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d571c0d-feb6-4c1c-8793-cd9d9585c40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_numel(m: nn.Module):\n",
    "    return sum(p.numel() for p in m.parameters())   \n",
    "\n",
    "def model_sz(m: nn.Module):\n",
    "    return sum(p.numel() * p.dtype.itemsize for p in m.parameters())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc220de1-d991-4a27-aa91-20ac800069eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL PARMS: 14_820_847_264\n",
      "TRANS PARMS: 1_550_624_768\n",
      "LINEAR PARMS: 333_971_456\n",
      "TOTAL MAMBA: 12_706_867_872\n",
      "=== SINGLE MAMBA === x 81\n",
      "TOTAL: 156_867_744\n",
      "IN: 105_398_272\n",
      "CONV: 74_240\n",
      "OUT: 51_380_224\n"
     ]
    }
   ],
   "source": [
    "def model_parm_count(count_fn=model_sz):\n",
    "    # NOTE: doesn't work with hqq\n",
    "    print(f\"TOTAL PARMS: {model_sz(model):_}\")\n",
    "    print(f\"TRANS PARMS: {count_fn(model.model.blocks):_}\")\n",
    "    print(f\"LINEAR PARMS: {count_fn(model.model.linear_layers):_}\")\n",
    "    print(f\"TOTAL MAMBA: {count_fn(model.model.mamba_layers):_}\")\n",
    "    print(f\"=== SINGLE MAMBA === x {len(model.model.mamba_layers)}\")\n",
    "    m = model.model.mamba_layers[0].mamba\n",
    "    print(f\"TOTAL: {count_fn(m):_}\")\n",
    "    print(f\"IN: {count_fn(m.in_proj):_}\")\n",
    "    print(f\"CONV: {count_fn(m.conv1d):_}\")\n",
    "    print(f\"OUT: {count_fn(m.out_proj):_}\")\n",
    "model_parm_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd8fb465-b046-4dcb-a274-5570f779c720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual function to convert nn.layer to hqq\n",
    "def do_quant(nn_linear, quant_config: Optional[BaseQuantizeConfig]):\n",
    "    if not quant_config:\n",
    "        return nn_linear    \n",
    "    assert isinstance(nn_linear, nn.Linear), f\"expected nn.Linear, got {type(nn_linear)}\"\n",
    "    hqq_layer = HQQLinear(nn_linear, \n",
    "                      quant_config=BaseQuantizeConfig(**quant_config, axis=AXIS),\n",
    "                      compute_dtype=torch.bfloat16,\n",
    "                      device='cuda', \n",
    "                      initialize=True, \n",
    "                      del_orig=True) \n",
    "    return hqq_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f4c400-0b29-42f2-88c9-63f9de214197",
   "metadata": {},
   "source": [
    "#  Actual quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3db4d6-b63e-49ec-aab5-0aa8a8cdaf00",
   "metadata": {},
   "source": [
    "## Block self attention\n",
    "\n",
    "Main points of interest: q_proj, k_proj, v_proj, o_proj.\n",
    "\n",
    "It also has all loras, but I didn't want to touch them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5b95b35-836e-40ad-92f1-5f39eca66b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qkvo_quant = NO_QUANT\n",
    "for block in model.model.blocks:\n",
    "    block = block.self_attn    \n",
    "    block.q_proj = do_quant(block.q_proj, qkvo_quant)\n",
    "    block.k_proj = do_quant(block.k_proj, qkvo_quant)\n",
    "    block.v_proj = do_quant(block.v_proj, qkvo_quant)\n",
    "    block.o_proj = do_quant(block.o_proj, qkvo_quant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9313e3c-1863-4480-a41d-3ad1aade5b83",
   "metadata": {},
   "source": [
    "## MLP\n",
    "\n",
    "It has shared linear_fc1(up+gate), linear_fc2(down), lora.\n",
    "First two are the main points of the interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54ec9bc4-b133-4a3c-974a-706a63c791ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-80): 81 x Zamba2MambaDecoderLayer(\n",
       "    (mamba): Mamba2Layer(\n",
       "      (in_proj): ModuleList(\n",
       "        (0): Linear(in_features=3584, out_features=14704, bias=False)\n",
       "      )\n",
       "      (conv1d): Conv1d(7424, 7424, kernel_size=(4,), stride=(1,), padding=(3,), groups=7424)\n",
       "      (act): SiLU()\n",
       "      (norm): RMSNorm()\n",
       "      (out_proj): Linear(in_features=7168, out_features=3584, bias=False)\n",
       "    )\n",
       "    (input_layernorm): Zamba2RMSNorm()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Without moving some memory to CPU we'll hit OoM\n",
    "model.model.mamba_layers.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e4e223f-9351-463c-866f-cd4942ccc6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_quant = NO_QUANT\n",
    "for block in model.model.blocks:\n",
    "    block = block.feed_forward\n",
    "    block.linear_fc1 = do_quant(block.linear_fc1, mlp_quant)\n",
    "    block.linear_fc2 = do_quant(block.linear_fc2, mlp_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3043525-62a5-49ac-adc7-8727e71774ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-80): 81 x Zamba2MambaDecoderLayer(\n",
       "    (mamba): Mamba2Layer(\n",
       "      (in_proj): ModuleList(\n",
       "        (0): Linear(in_features=3584, out_features=14704, bias=False)\n",
       "      )\n",
       "      (conv1d): Conv1d(7424, 7424, kernel_size=(4,), stride=(1,), padding=(3,), groups=7424)\n",
       "      (act): SiLU()\n",
       "      (norm): RMSNorm()\n",
       "      (out_proj): Linear(in_features=7168, out_features=3584, bias=False)\n",
       "    )\n",
       "    (input_layernorm): Zamba2RMSNorm()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.mamba_layers.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca40c958-78e8-402e-ac25-b89e414d3c4e",
   "metadata": {},
   "source": [
    "## Additional layers\n",
    "\n",
    "Zamba has additional linear layers for transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80db5a36-4a37-487d-b8ec-995c11840959",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_quant = NO_QUANT\n",
    "\n",
    "block = model.model.linear_layers\n",
    "for i in range(len(block)):\n",
    "    block[i] = do_quant(block[i], lin_quant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c24c74-ca77-462a-b1cb-3405a11d3c10",
   "metadata": {},
   "source": [
    "## Mamba.\n",
    "\n",
    "Mamba has two linear blocks: in_proj, out_proj.\n",
    "It also has conv1d, but quanting conv it would requrie switching to even less efficient path course and there are next to no parms in conv, while in_proj consumes the majority of space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61255afa-6c8f-4b07-937c-7e0d2d5aaed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "block = model.model.mamba_layers\n",
    "\n",
    "def nth(n): return block[n].mamba\n",
    "\n",
    "GENERAL_QUANT = BETTER_QUANT\n",
    "i = 0\n",
    "for i in range(i, len(block)):\n",
    "    nth(i).in_proj[0] = do_quant(nth(i).in_proj[0], GENERAL_QUANT)\n",
    "    nth(i).out_proj = do_quant(nth(i).out_proj, GENERAL_QUANT)\n",
    "    i += 1\n",
    "    \n",
    "assert i == len(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e83489c-cbfa-4b35-88b5-5222c89adb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWarning: failed to import the Marlin backend. Check if marlin is correctly installed if you want to use the Marlin backend (https://github.com/IST-DASLab/marlin).\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if USE_AOINT4:\n",
    "    from hqq.utils.patching import prepare_for_inference\n",
    "    prepare_for_inference(model, backend=\"torchao_int4\") \n",
    "else:\n",
    "    HQQLinear.set_backend(HQQBackend.ATEN_FORWARD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a240c43-5775-4804-adfc-27b59a30fab4",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b729a30-2fb5-4254-ad47-89734ca94f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31fd455c-05c3-42e7-86e6-d9dff36dd8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling_zamba2 cache implementation doesn't work with non-forked vesrion\n",
    "# as it doesn't inherit from Cache, which breaks the generation.\n",
    "class FixedHybraCache(modeling_zamba2.HybridMambaAttentionDynamicCache, transformers.Cache):\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fabeeb05-550d-4c1e-b5f9-efe445113163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt = \"What factors contributed to the fall of the Roman Empire?\"\n",
    "#prompt = \"Give me a list of good god-tier reasons why kittehs are cuter than doggos. Make it verbose: several sentences per item.\"\n",
    "prompt = \"Write a long 1000 words story about dreams\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee33fb02-7d1b-400a-8c4d-c035f705966e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Write a long 1000 words story about dreams<|im_end|>\n",
      "\n",
      "*** TIME: 65.19050335884094, TPS:15.615771432173515 cache_exist=True, |T|:1018\n",
      "<|im_start|> user\n",
      "Write a long 1000 words story about dreams<|im_end|> \n",
      "<|im_start|> assistant\n",
      "Once upon a time, in a small village nestled between rolling hills and lush forests, there lived a young girl named Lila. Lila was a dreamer, always lost in her own world of imagination and wonder. She had a special gift - the ability to enter the realm of dreams.\n",
      "\n",
      "Every night, as she drifted off to sleep, Lila would find herself transported to a magical land filled with vibrant colors, enchanting creatures, and breathtaking landscapes. In this dream world, she could fly through the skies, explore hidden caves, and converse with wise old sages. It was a place where anything was possible, and Lila reveled in the freedom and joy it brought her.\n",
      "\n",
      "One evening, as Lila lay in bed, she felt a strange sensation wash over her. The dream world seemed different somehow, more alive and vivid than ever before. As she ventured deeper into this new realm, she noticed a faint glow emanating from a distant mountain peak. Intrigued, Lila set off towards the light, her heart filled with curiosity and excitement.\n",
      "\n",
      "As she climbed higher, the air grew colder, and the path became increasingly treacherous. Lila's determination, however, never wavered. She pressed on, driven by an unwavering belief that the mountain held a secret waiting to be uncovered.\n",
      "\n",
      "Finally, after what felt like an eternity, Lila reached the summit. Before her stood a magnificent crystal palace, its walls shimmering with an ethereal light. The entrance was guarded by a majestic dragon, its scales glistening like precious gems. Lila approached the creature with reverence, sensing that it was not a threat but a protector of this sacred place.\n",
      "\n",
      "With a gentle nod, the dragon allowed Lila to pass. As she stepped inside the palace, she found herself in a grand hall filled with ancient artifacts and mysterious relics. At the far end of the room stood an ornate throne, upon which sat an enigmatic figure cloaked in shadows.\n",
      "\n",
      "\"Who are you?\" Lila asked, her voice trembling with a mix of awe and fear.\n",
      "\n",
      "The figure slowly rose from the throne, revealing a wise and benevolent old woman. Her eyes sparkled with kindness, and her voice carried the weight of centuries.\n",
      "\n",
      "\"I am the Dreamweaver,\" she said, her words echoing through the hall. \"I have been waiting for you, Lila. You possess a rare gift - the ability to enter the realm of dreams and shape its very fabric.\"\n",
      "\n",
      "Lila's eyes widened in astonishment. She had always known that her dreams were special, but she had never imagined that they held such power.\n",
      "\n",
      "The Dreamweaver continued, \"Your dreams are a tapestry, woven from the threads of your imagination and the desires of your heart. But there are those who seek to unravel this tapestry, to control and manipulate the dreams of others for their own gain.\"\n",
      "\n",
      "Lila's heart sank at the thought of such a dark and sinister force. \"What can I do?\" she asked, her voice filled with determination.\n",
      "\n",
      "The Dreamweaver smiled warmly. \"You have the power to protect the dreams of others, to weave a protective shield around their subconscious minds. But to do so, you must first master the art of dreamwalking - the ability to navigate the realms of the dream world with precision and control.\"\n",
      "\n",
      "And so, Lila embarked on a journey of self-discovery and growth. Under the guidance of the Dreamweaver, she learned to harness her gift, to traverse the dream world with grace and purpose. She encountered fantastical creatures, faced her deepest fears, and unlocked the secrets of her own subconscious mind.\n",
      "\n",
      "As Lila's skills grew, so did her resolve. She knew that she had a responsibility to protect the dreams of others, to safeguard the sanctity of the dream world. And so, she set out on a quest to confront the forces that sought to exploit and control the dreams of innocent people.\n",
      "\n",
      "With each victory, Lila's legend grew. She became known as the Dream Guardian, a hero who fought to preserve the beauty and wonder of the dream world. And as she continued to weave her protective shield around the dreams of others, she discovered that her own dreams had become more vivid and vibrant than ever before.\n",
      "\n",
      "In the end, Lila realized that the true power of her gift lay not in the ability to control the dreams of others, but in the ability to\n"
     ]
    }
   ],
   "source": [
    "def run_model(prompt, max_new_tokens=150, is_instruct=True):\n",
    "    if is_instruct:\n",
    "        sample = [{'role': 'user', 'content': prompt}]\n",
    "        prompt = tokenizer.apply_chat_template(sample, tokenize=False)\n",
    "    print(prompt)    \n",
    "    input_ids = tokenizer(prompt, return_tensors='pt', add_special_tokens=False).to(\"cuda\")\n",
    "    start_time = time.time()    \n",
    "    cache=FixedHybraCache(model.config, 1)\n",
    "    cache_exist = cache is not None\n",
    "    outputs = model.generate(\n",
    "        **input_ids, \n",
    "        max_new_tokens=1000, #1000\n",
    "        return_dict_in_generate=False, \n",
    "        output_scores=False,     \n",
    "        num_beams=1, \n",
    "        use_cache=cache is not None,\n",
    "        past_key_values=cache,\n",
    "        do_sample=False)\n",
    "    delta_time = time.time() - start_time\n",
    "    tps = outputs.numel() / delta_time\n",
    "    print(f\"*** TIME: {delta_time}, TPS:{tps} {cache_exist=}, |T|:{outputs.numel()}\")\n",
    "    print((tokenizer.decode(outputs[0])))\n",
    "\n",
    "run_model(prompt)\n",
    "\n",
    "# ATEN: *** TIME: 305.0571117401123, TPS:3.2485720275364827 cache_exist=True, |T|:991\n",
    "# AO4:  *** TIME: 60.70746970176697, TPS:16.768941367529436 cache_exist=True, |T|:1018\n",
    "# RAW:  *** TIME: 57.77293086051941, TPS:15.560920773959806 cache_exist=True, |T|:899"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "146f0709-f033-4ded-b94b-31d6745408b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Zamba2ForCausalLM(\n",
       "  (model): Zamba2Model(\n",
       "    (embed_tokens): Embedding(32000, 3584, padding_idx=0)\n",
       "    (blocks): ModuleList(\n",
       "      (0-1): 2 x Zamba2AttentionDecoderLayer(\n",
       "        (self_attn): Zamba2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=7168, out_features=7168, bias=False)\n",
       "          (k_proj): Linear(in_features=7168, out_features=7168, bias=False)\n",
       "          (v_proj): Linear(in_features=7168, out_features=7168, bias=False)\n",
       "          (o_proj): Linear(in_features=7168, out_features=3584, bias=False)\n",
       "          (rotary_emb): Zamba2RotaryEmbedding()\n",
       "        )\n",
       "        (feed_forward): Zamba2MLP(\n",
       "          (linear_fc1): Linear(in_features=3584, out_features=28672, bias=False)\n",
       "          (linear_fc2): Linear(in_features=14336, out_features=3584, bias=False)\n",
       "          (linear_fc1_lora_A_list): ParameterList(\n",
       "              (0): Object of type: Linear\n",
       "              (1): Object of type: Linear\n",
       "              (2): Object of type: Linear\n",
       "              (3): Object of type: Linear\n",
       "              (4): Object of type: Linear\n",
       "              (5): Object of type: Linear\n",
       "              (6): Object of type: Linear\n",
       "              (7): Object of type: Linear\n",
       "              (8): Object of type: Linear\n",
       "              (9): Object of type: Linear\n",
       "              (10): Object of type: Linear\n",
       "              (11): Object of type: Linear\n",
       "              (12): Object of type: Linear\n",
       "            (0): Linear(in_features=3584, out_features=128, bias=False)\n",
       "            (1): Linear(in_features=3584, out_features=128, bias=False)\n",
       "            (2): Linear(in_features=3584, out_features=128, bias=False)\n",
       "            (3): Linear(in_features=3584, out_features=128, bias=False)\n",
       "            (4): Linear(in_features=3584, out_features=128, bias=False)\n",
       "            (5): Linear(in_features=3584, out_features=128, bias=False)\n",
       "            (6): Linear(in_features=3584, out_features=128, bias=False)\n",
       "            (7): Linear(in_features=3584, out_features=128, bias=False)\n",
       "            (8): Linear(in_features=3584, out_features=128, bias=False)\n",
       "            (9): Linear(in_features=3584, out_features=128, bias=False)\n",
       "            (10): Linear(in_features=3584, out_features=128, bias=False)\n",
       "            (11): Linear(in_features=3584, out_features=128, bias=False)\n",
       "            (12): Linear(in_features=3584, out_features=128, bias=False)\n",
       "          )\n",
       "          (linear_fc1_lora_B_list): ParameterList(\n",
       "              (0): Object of type: Linear\n",
       "              (1): Object of type: Linear\n",
       "              (2): Object of type: Linear\n",
       "              (3): Object of type: Linear\n",
       "              (4): Object of type: Linear\n",
       "              (5): Object of type: Linear\n",
       "              (6): Object of type: Linear\n",
       "              (7): Object of type: Linear\n",
       "              (8): Object of type: Linear\n",
       "              (9): Object of type: Linear\n",
       "              (10): Object of type: Linear\n",
       "              (11): Object of type: Linear\n",
       "              (12): Object of type: Linear\n",
       "            (0): Linear(in_features=128, out_features=28672, bias=False)\n",
       "            (1): Linear(in_features=128, out_features=28672, bias=False)\n",
       "            (2): Linear(in_features=128, out_features=28672, bias=False)\n",
       "            (3): Linear(in_features=128, out_features=28672, bias=False)\n",
       "            (4): Linear(in_features=128, out_features=28672, bias=False)\n",
       "            (5): Linear(in_features=128, out_features=28672, bias=False)\n",
       "            (6): Linear(in_features=128, out_features=28672, bias=False)\n",
       "            (7): Linear(in_features=128, out_features=28672, bias=False)\n",
       "            (8): Linear(in_features=128, out_features=28672, bias=False)\n",
       "            (9): Linear(in_features=128, out_features=28672, bias=False)\n",
       "            (10): Linear(in_features=128, out_features=28672, bias=False)\n",
       "            (11): Linear(in_features=128, out_features=28672, bias=False)\n",
       "            (12): Linear(in_features=128, out_features=28672, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): Zamba2RMSNorm()\n",
       "        (pre_ff_layernorm): Zamba2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (mamba_layers): ModuleList(\n",
       "      (0-80): 81 x Zamba2MambaDecoderLayer(\n",
       "        (mamba): Mamba2Layer(\n",
       "          (in_proj): ModuleList(\n",
       "            (0): HQQLinearTorchWeightOnlynt4()\n",
       "          )\n",
       "          (conv1d): Conv1d(7424, 7424, kernel_size=(4,), stride=(1,), padding=(3,), groups=7424)\n",
       "          (act): SiLU()\n",
       "          (norm): RMSNorm()\n",
       "          (out_proj): HQQLinearTorchWeightOnlynt4()\n",
       "        )\n",
       "        (input_layernorm): Zamba2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (linear_layers): ModuleList(\n",
       "      (0-12): 13 x Linear(in_features=3584, out_features=3584, bias=False)\n",
       "    )\n",
       "    (final_layernorm): Zamba2RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
